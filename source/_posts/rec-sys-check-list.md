---
title: 推荐算法效果不佳时的检查清单
date: 2022-03-11 22:42:34
categories: [推荐系统]
tags: [推荐算法]
---
有时候我们会遇到推荐算法上线之后，效果不如预期的情况。这种情况下，该如何改进呢？

下面就尝试列出一些检查清单，按照重要性的顺序，建议从上往下依次检查。当然，这些清单还不全面，欢迎大家一起来补充！

## 1. 用来统计效果指标的行为日志是否正确

- [ ] 是否准确、完整地收集了影响统计指标的行为日志？
- [ ] 埋点是否正确、完整？新的埋点逻辑是否有足够覆盖率的APP的版本更新？

**错误Case**:

- 之前有一个客户把实验桶标识存储在Web页面的cookie里，在不同推荐场景的页面之间跳转时，收集到日志服务器的实验桶标识发生了错乱。
- 还是这个客户，只在当前推荐场景的第一页收集了实验埋点，从推荐结果的第二页开始就没有收集埋点信息，导致效果统计时第二页之后的流量都算到基准桶里了。
- 当推荐服务超时或异常时，APP端会出一个兜底的默认推荐列表；这个时候实验桶标记既不应该记录为算法桶标记，也不应该记录为基准桶标记，而应该记录为一个特殊的'other'标记，否则在统计效果的时候就会发现有些用户既存在于算法桶内又存在于基准桶内的矛盾。

## 2. 流量切分是否稳定、随机和足量

- [ ] 流量切分是否是稳定的，即同一个用户在实验期间是否只能落在同一个桶内？
- [ ] 流量切分是否足够随机，会不会存在某些行为异常的用户集中在同一个桶内？对调实验桶的用户集合是否能够观察到对应的效果指标的翻转？
- [ ] AA测试的效果是否是比较接近的？当链路上出现系统性错误时，通常会通知AA测试的效果差异很大。
- [ ] 在统计周期内，实验桶的流量是否足够得出置信的统计结果？这里推荐一个流量多少（样本数量）与指标置信度之间换算的计算器：[https://www.evanmiller.org/ab-testing/sample-size.html](https://www.evanmiller.org/ab-testing/sample-size.html)。如果实验桶流量不够，则需要拉长统计周期。
<!--more-->
**错误Case**:

- 一些小流量的推荐场景，按天为周期统计指标得到的结论是不可靠的，应根据流量情况适当拉长统计周期；以短周期的效果指标做决策是有风险的。


## 3. 统计效果指标的代码是否正确

- [ ] 是否筛选了目标场景的日志，过滤掉了无关日志和噪音数据？
- [ ] 统计代码实现是否正确？如何处理低版本的APP流量收集不到正确埋点数据的问题？
- [ ] 各实验桶的流量统计结果占比是否和流量切分的比例大致相同？

**错误Case**:

- 统计到的流量切分比例跟实际的切分比例不一致的情况，还是经常出现的。很可能是流量切分的逻辑不合理，或者日志回流的链路上存在问题。

## 4. 实验变量控制是否正确

- [ ] 基准桶和实验桶是否做到了只有观察变量（比如推荐策略）的不同，而其他方面完全相同？
- [ ] 扩展上一个问题：基准桶和实验桶的推荐候选集是完全一样的吗？
- [ ] 扩展上一个问题：基准桶和实验桶的推荐候选集是否存在更新步调不一致的问题？
- [ ] 扩展上一个问题：基准桶和实验桶的候选集过滤规则（在工程链路的实现上）是完全一样的吗？

**错误Case**:

- 基准桶和实验桶的推荐候选集不一致，是推荐系统刚上线时经常遇到的问题

## 5. 工程链路是否正确，是否能做到持续正确

- [ ] 推荐服务的逻辑实现是否正确？有bug么？如何验证？
- [ ] 推荐服务调用的子系统（如，模型服务、在线存储、实时计算等）是否正确响应？是否存在问题？
- [ ] 依赖的各种离线、近线、实时数据更新是否正常？更新的数据是否正确？
- [ ] 推荐服务的性能是否足够，会不会存在高峰期服务不能正常响应的问题？
- [ ] 推荐服务的日志中有错误Log么？这些错误Log会影响推荐效果指标么？子系统有错误Log么？
- [ ] 上面这些因素是否会随时间发生变化？
- [ ] 系统上的各类指标（性能、cpu、内存、数据量、效果指标等）有监控和报警机制么？

## 6. 算法模型有效么？并且持续有效么？

- [ ] 是否存在特征数据穿透的问题？
- [ ] 是否考虑了各种系统偏差？如数据循环、信息茧房的问题。
- [ ] 召回模型是否做了负样本采样？
- [ ] 模型是否做了离线效果指标的验证？验证数据集合理么（是否独立于训练集）？是否存在欠拟合、过拟合？
- [ ] 新模型上线是否做了A/B测试，并在验证效果ok后才推广到全流量？
- [ ] 离线、在线的特征工程的结果是否一致？有验证么？
- [ ] 模型离线预测结果与在线预测结果是否一致？
- [ ] 模型如何平衡多个目标，是否存在跷跷板现象？
- [ ] 特征和模型是否及时更新？随着时间的推移，用户群体、用户行为偏好、物品集的分布都会发生变化，特征和模型如果不及时更新效果就会慢慢衰减。特征上，要注意做数值特征的离散化时的分箱边界（boundary）是否更新了，等等诸如此类的问题。

## 7. 推荐策略是否有瑕疵？

- [ ] 召回结果的多样性是否足够？
- [ ] 召回结果的覆盖率是否足够？是否考虑了冷启动的用户和物品？
- [ ] 召回结果的截断方法是否合理，是否破坏了多样性、覆盖率、探索机制、冷启动策略等？
- [ ] 如有粗排模型，粗排模型的作用范围是否合理？是否破坏了多样性、覆盖率、探索机制、冷启动策略等？
- [ ] 精排之后是否有满足业务期望的重排策略？比如，多样性打散算法或规则？

## 8. 是否考虑了推荐策略对业务长期受益的影响？

- [ ] 用户、物品的冷启动问题有考虑并建模么？
- [ ] 是否平衡了Exploration & Exploitation的问题？
- [ ] 是否有必要对特定候选集物品做流量调控？
- [ ] 推荐策略是否照顾到了各方利益（用户、内容提供者、平台、广告商等）
